package="expanded", check.size=F,
token=NEON_TOKEN)
# set working directory
wd <- "~/data" # this will depend on your local machine
setwd(wd)
setwd("C:/Users/Ope4/OneDrive - Northern Arizona University/Desktop/ACADEMIC_SEMSTER/SPRING 2026/INF_550/WEEK 5/NEON_DATA")
foliar <- loadByProduct(dpID="DP1.10026.001", site="all",
package="expanded", check.size=F,
token=NEON_TOKEN)
setwd("C:/Users/Ope4/NEON_DATA")
setwd("C:/Users/Ope4/NEON_DATA")
foliar <- loadByProduct(dpID="DP1.10026.001", site="all",
package="expanded", check.size=F,
token=NEON_TOKEN)
foliar <- loadByProduct(dpID="DP1.10026.001", site="all",
package="expanded", check.size=F,
token=NEON_TOKEN,
savepath = "C:/Users/Ope4/NEON_DATA")
neon_download <- loadByProduct(
dpID = "DP4.00200.001",  # Eddy covariance fluxes
site = "xCP",
startdate = "2023-01",
enddate = "2024-12",
package = "basic",
token=NEON_TOKEN
)
zipsByProduct(
dpID = "DP4.00200.001",
site = "xCP",
startdate = "2023-01",
enddate = "2024-12",
package = "basic"
)
# install packages if needed
install.packages(c("amerifluxr", "neonUtilities", "tidyverse", "lubridate"))
# install packages if needed
install.packages(c("amerifluxr", "neonUtilities", "tidyverse", "lubridate"))
library(amerifluxr)
library(neonUtilities)
library(tidyverse)
library(lubridate)
# metScanR comes from GitHub
if (!requireNamespace("devtools", quietly = TRUE)) install.packages("devtools")
# install.packages("remotes") # if not installed yet
remotes::install_github("ua-snap/amerifluxr")
install.packages("amerifluxr")
library("amerifluxr")
if(!require(devtools)){install.packages("devtools")}
devtools::install_github("chuhousen/amerifluxr")
library(amerifluxr)
library(neonUtilities)
library(tidyverse)
library(lubridate)
library("amerifluxr")
# See AmeriFlux sites; NEON-in-AmeriFlux sites start with 'US-x'
amf_sites <- amf_site_info()
View(amf_sites)
neon_amf <- dplyr::filter(amf_sites, stringr::str_detect(SITE_ID, "^US-x"))
neon_amf %>% dplyr::select(SITE_ID, SITE_NAME, IGBP)
View(neon_amf)
# =======================
# 0) Setup
# =======================
if (!requireNamespace("BiocManager", quietly = TRUE)) install.packages("BiocManager")
if (!requireNamespace("rhdf5", quietly = TRUE)) BiocManager::install("rhdf5")
library(amerifluxr)     # AmeriFlux API & files  (download/read BASE)  (docs)  [5](https://chuhousen.github.io/amerifluxr/)
library(neonUtilities)  # NEON dp04 HDF5 extraction via stackEddy()    (docs)  [2](https://www.neonscience.org/resources/learning-hub/tutorials/eddy-data-intro)
library(lubridate); library(dplyr); library(ggplot2)
# =======================
# 1) Download AmeriFlux BASE for US-xHA
# =======================
user  <- "Ogonna"
email <- "Ogonnaeli@yahoo.com"
password: "Miracle2009+++"
password <- "Miracle2009+++"
base_paths <- amf_download_base(
user_id = user, user_email = email,
site_id = amf_site, data_product = "BASE-BADM",
data_policy = "CCBY4.0", agree_policy = TRUE,
intended_use = "education",
intended_use_text = "Extending US-xHA with NEON dp04 for class exercise",
out_dir = tempdir(), verbose = TRUE
) # API and function docs: amerifluxr  [6](https://search.r-project.org/CRAN/refmans/amerifluxr/html/amf_download_base.html)
# Site choices for this run
amf_site  <- "US-xHA"   # AmeriFlux ID for Harvard Forest
neon_site <- "HARV"     # NEON code mapped to US-xHA  (official mapping)           [1](https://www.neonscience.org/impact/observatory-blog/gap-filled-and-partitioned-neon-data-products-available-part-ameriflux)
site_tz   <- "America/New_York"
utc_offset_hours <- -5  # Eastern Standard Time offset for AmeriFlux "local standard time" (no DST).  [3](https://ameriflux.lbl.gov/data/uploading-half-hourly-hourly-data/)
base_paths <- amf_download_base(
user_id = user, user_email = email,
site_id = amf_site, data_product = "BASE-BADM",
data_policy = "CCBY4.0", agree_policy = TRUE,
intended_use = "education",
intended_use_text = "Extending US-xHA with NEON dp04 for class exercise",
out_dir = tempdir(), verbose = TRUE
) # API and function docs: amerifluxr  [6](https://search.r-project.org/CRAN/refmans/amerifluxr/html/amf_download_base.html)
amf_base <- amf_read_base(base_paths) %>%          # parses BASE file to data frame  [7](https://www.rdocumentation.org/packages/amerifluxr/versions/1.0.0)
select(TIMESTAMP_START, TIMESTAMP_END, FC, H, LE)
# Convert AmeriFlux local standard times -> UTC to know where to extend.
# (UTC = local_standard + |utc_offset_hours| hours; for EST, +5h)
amf_last_end_utc <- ymd_hm(as.character(max(amf_base$TIMESTAMP_END)), tz = "UTC") +
hours(+abs(utc_offset_hours))
# =======================
# 2) Download NEON dp04 HDF5 *after* AmeriFlux coverage
# =======================
start_month <- format(amf_last_end_utc + days(1), "%Y-%m")
end_month   <- format(Sys.Date(), "%Y-%m")
save_dir <- file.path(tempdir(), "neon_dp04_usxha")
dir.create(save_dir, showWarnings = FALSE)
zipsByProduct(                                     # NEON Data API bundle downloader  [2](https://www.neonscience.org/resources/learning-hub/tutorials/eddy-data-intro)
dpID = "DP4.00200.001", site = neon_site,
startdate = start_month, enddate = end_month,
package = "basic", savepath = save_dir, check.size = FALSE
)
flux_list <- stackEddy(
filepath = file.path(save_dir, "filesToStack00200"),
level    = "dp04", metadata = TRUE
) # returns list: [[HARV]], variables, objDesc         [2](https://www.neonscience.org/resources/learning-hub/tutorials/eddy-data-intro)
neon_dp04 <- flux_list[[neon_site]]
# =======================
# 3) Map NEON dp04 -> AmeriFlux variables & timestamps
# =======================
# NEON dp04 flux components (turbulent) → AmeriFlux FC/H/LE; times are UTC.
# (dp04 variables per NEON tutorial)  [2](https://www.neonscience.org/resources/learning-hub/tutorials/eddy-data-intro)
neon_amf_like <- neon_dp04 %>%
transmute(
timeBgn_UTC = timeBgn, timeEnd_UTC = timeEnd,
FC = `data.fluxCo2.turb.flux`,     # µmol CO2 m-2 s-1
H  = `data.fluxTemp.turb.flux`,    # W m-2
LE = `data.fluxH2o.turb.flux`      # W m-2
) %>%
# Convert to AmeriFlux timestamp strings in *local standard time* (no DST)  [3](https://ameriflux.lbl.gov/data/uploading-half-hourly-hourly-data/)
mutate(
TIMESTAMP_START = format(with_tz(timeBgn_UTC, site_tz), "%Y%m%d%H%M"),
TIMESTAMP_END   = format(with_tz(timeEnd_UTC,   site_tz), "%Y%m%d%H%M")
) %>%
select(TIMESTAMP_START, TIMESTAMP_END, FC, H, LE) %>%
arrange(TIMESTAMP_START)
# Filter NEON rows that are strictly *after* the AmeriFlux coverage (compare in UTC)
neon_after <- neon_dp04 %>%
filter(timeEnd > amf_last_end_utc) %>%
transmute(
TIMESTAMP_START = format(with_tz(timeBgn, site_tz), "%Y%m%d%H%M"),
TIMESTAMP_END   = format(with_tz(timeEnd, site_tz), "%Y%m%d%H%M"),
FC = `data.fluxCo2.turb.flux`,
H  = `data.fluxTemp.turb.flux`,
LE = `data.fluxH2o.turb.flux`
) %>% arrange(TIMESTAMP_START)
# =======================
# 4) Append to BASE and plot FC/H
# =======================
extended <- bind_rows(
amf_base %>% select(TIMESTAMP_START, TIMESTAMP_END, FC, H, LE),
neon_after
) %>% arrange(TIMESTAMP_START)
# Plots
ggplot(extended, aes(x = ymd_hm(TIMESTAMP_START), y = FC)) +
geom_line(color="#1b9e77", linewidth=0.3) +
labs(x=NULL, y="FC (µmol m⁻² s⁻¹)", title=paste0(amf_site, " — Extended FC time series"))
ggplot(extended, aes(x = ymd_hm(TIMESTAMP_START), y = H)) +
geom_line(color="#d95f02", linewidth=0.3) +
labs(x=NULL, y="H (W m⁻²)", title=paste0(amf_site, " — Extended H time series"))
# ---- 3) Standardize AmeriFlux timestamps to character (12-digit) ----
amf_base <- amf_base %>%
mutate(
TIMESTAMP_START = sprintf("%012.0f", as.numeric(TIMESTAMP_START)),
TIMESTAMP_END   = sprintf("%012.0f", as.numeric(TIMESTAMP_END))
)
# ---- 4) Rebuild NEON → AmeriFlux timestamps with *fixed* UTC offset (no DST) ----
# NEON dp04 times are UTC; AmeriFlux requires local standard time (EST = UTC - 5h).
utc_offset_hours <- -5  # US-xHA (Harvard Forest) standard offset
neon_after <- neon_dp04 %>%
filter(timeEnd > amf_last_end_utc) %>%         # this 'amf_last_end_utc' you already created
transmute(
TIMESTAMP_START = format(timeBgn + hours(utc_offset_hours), "%Y%m%d%H%M"),
TIMESTAMP_END   = format(timeEnd + hours(utc_offset_hours), "%Y%m%d%H%M"),
FC = `data.fluxCo2.turb.flux`,               # µmol CO2 m-2 s-1 (AmeriFlux FC)
H  = `data.fluxTemp.turb.flux`,              # W m-2 (AmeriFlux H)
LE = `data.fluxH2o.turb.flux`                # W m-2 (AmeriFlux LE)
) %>%
arrange(TIMESTAMP_START)
# ---- 5) Bind rows now that both sides are character timestamps ----
extended <- bind_rows(
amf_base %>% select(TIMESTAMP_START, TIMESTAMP_END, FC, H, LE),
neon_after
) %>% arrange(TIMESTAMP_START)
# ---- 6) Plot (parse timestamps for display only) ----
# These timestamps are in *local standard time*; parse as such for plotting.
site_tz <- "America/New_York"
p_fc <- ggplot(extended, aes(x = ymd_hm(TIMESTAMP_START, tz = site_tz), y = FC)) +
geom_line(color="#1b9e77", linewidth=0.3) +
labs(x=NULL, y="FC (µmol m⁻² s⁻¹)", title=paste0("US-xHA — Extended FC time series"))
p_h <- ggplot(extended, aes(x = ymd_hm(TIMESTAMP_START, tz = site_tz), y = H)) +
geom_line(color="#d95f02", linewidth=0.3) +
labs(x=NULL, y="H (W m⁻²)", title=paste0("US-xHA — Extended H time series"))
p_fc; p_h
# =======================
# 5) Use metScanR to find/verify co-located sites
# =======================
if (!requireNamespace("devtools", quietly=TRUE)) install.packages("devtools")
if (!requireNamespace("metScanR", quietly=TRUE)) devtools::install_github("jaroberti/metScanR")
library(metScanR)
# =======================
# 5) Use metScanR to find/verify co-located sites
# =======================
#install metScanR:
install.packages("metScanR")
#load the package:
library(metScanR)
if (!requireNamespace("metScanR", quietly=TRUE)) devtools::install_github("jaroberti/metScanR")
library(metScanR)
#install metScanR:
install.packages("metScanR")
#load the package:
library(metScanR)
#download and save the most up-to-date database:
updateDatabase()
setwd("C:/Users/Ope4/OneDrive - Northern Arizona University/Desktop/ACADEMIC_SEMSTER/SPRING 2026/INF_550/WEEK 5")
# WEEK 5A
# Site choices for this run
amf_site  <- "US-xHA"   # AmeriFlux ID for Harvard Forest
neon_site <- "HARV"     # NEON code mapped to US-xHA  (official mapping)           [1](https://www.neonscience.org/impact/observatory-blog/gap-filled-and-partitioned-neon-data-products-available-part-ameriflux)
site_tz   <- "America/New_York"
utc_offset_hours <- -5  # Eastern Standard Time offset for AmeriFlux "local standard time" (no DST).  [3](https://ameriflux.lbl.gov/data/uploading-half-hourly-hourly-data/)
# =======================
# 1) Download AmeriFlux BASE for US-xHA
# =======================
user  <- "Ogonna"
email <- "Ogonnaeli@yahoo.com"
password <- "Miracle2009+++"
base_paths <- amf_download_base(
user_id = user, user_email = email,
site_id = amf_site, data_product = "BASE-BADM",
data_policy = "CCBY4.0", agree_policy = TRUE,
intended_use = "education",
intended_use_text = "Extending US-xHA with NEON dp04 for class exercise",
out_dir = tempdir(), verbose = TRUE
) # API and function docs: amerifluxr  [6](https://search.r-project.org/CRAN/refmans/amerifluxr/html/amf_download_base.html)
amf_base <- amf_read_base(base_paths) %>%          # parses BASE file to data frame  [7](https://www.rdocumentation.org/packages/amerifluxr/versions/1.0.0)
select(TIMESTAMP_START, TIMESTAMP_END, FC, H, LE)
# Convert AmeriFlux local standard times -> UTC to know where to extend.
# (UTC = local_standard + |utc_offset_hours| hours; for EST, +5h)
amf_last_end_utc <- ymd_hm(as.character(max(amf_base$TIMESTAMP_END)), tz = "UTC") +
hours(+abs(utc_offset_hours))
# =======================
# 2) Download NEON dp04 HDF5 *after* AmeriFlux coverage
# =======================
start_month <- format(amf_last_end_utc + days(1), "%Y-%m")
end_month   <- format(Sys.Date(), "%Y-%m")
save_dir <- file.path(tempdir(), "neon_dp04_usxha")
dir.create(save_dir, showWarnings = FALSE)
zipsByProduct(                                     # NEON Data API bundle downloader  [2](https://www.neonscience.org/resources/learning-hub/tutorials/eddy-data-intro)
dpID = "DP4.00200.001", site = neon_site,
startdate = start_month, enddate = end_month,
package = "basic", savepath = save_dir, check.size = FALSE
)
flux_list <- stackEddy(
filepath = file.path(save_dir, "filesToStack00200"),
level    = "dp04", metadata = TRUE
) # returns list: [[HARV]], variables, objDesc         [2](https://www.neonscience.org/resources/learning-hub/tutorials/eddy-data-intro)
neon_dp04 <- flux_list[[neon_site]]
# ---- 3) Standardize AmeriFlux timestamps to character (12-digit) ----
amf_base <- amf_base %>%
mutate(
TIMESTAMP_START = sprintf("%012.0f", as.numeric(TIMESTAMP_START)),
TIMESTAMP_END   = sprintf("%012.0f", as.numeric(TIMESTAMP_END))
)
# ---- 4) Rebuild NEON → AmeriFlux timestamps with *fixed* UTC offset (no DST) ----
# NEON dp04 times are UTC; AmeriFlux requires local standard time (EST = UTC - 5h).
utc_offset_hours <- -5  # US-xHA (Harvard Forest) standard offset
neon_after <- neon_dp04 %>%
filter(timeEnd > amf_last_end_utc) %>%         # this 'amf_last_end_utc' you already created
transmute(
TIMESTAMP_START = format(timeBgn + hours(utc_offset_hours), "%Y%m%d%H%M"),
TIMESTAMP_END   = format(timeEnd + hours(utc_offset_hours), "%Y%m%d%H%M"),
FC = `data.fluxCo2.turb.flux`,               # µmol CO2 m-2 s-1 (AmeriFlux FC)
H  = `data.fluxTemp.turb.flux`,              # W m-2 (AmeriFlux H)
LE = `data.fluxH2o.turb.flux`                # W m-2 (AmeriFlux LE)
) %>%
arrange(TIMESTAMP_START)
# ---- 5) Bind rows now that both sides are character timestamps ----
extended <- bind_rows(
amf_base %>% select(TIMESTAMP_START, TIMESTAMP_END, FC, H, LE),
neon_after
) %>% arrange(TIMESTAMP_START)
# ---- 6) Plot (parse timestamps for display only) ----
# These timestamps are in *local standard time*; parse as such for plotting.
site_tz <- "America/New_York"
p_fc <- ggplot(extended, aes(x = ymd_hm(TIMESTAMP_START, tz = site_tz), y = FC)) +
geom_line(color="#1b9e77", linewidth=0.3) +
labs(x=NULL, y="FC (µmol m⁻² s⁻¹)", title=paste0("US-xHA — Extended FC time series"))
p_h <- ggplot(extended, aes(x = ymd_hm(TIMESTAMP_START, tz = site_tz), y = H)) +
geom_line(color="#d95f02", linewidth=0.3) +
labs(x=NULL, y="H (W m⁻²)", title=paste0("US-xHA — Extended H time series"))
p_fc; p_h
# ---- 0) Packages & install helpers -------------------------
req <- c("dplyr","ggplot2","lubridate","stringr")
for (p in req) if (!requireNamespace(p, quietly=TRUE)) install.packages(p)
lapply(req, library, character.only = TRUE)
if (!requireNamespace("amerifluxr", quietly=TRUE)) install.packages("amerifluxr")
library(amerifluxr)  # AmeriFlux API (sites/BASE)  [docs: amf_site_info, amf_download_base, amf_read_base]
if (!requireNamespace("neonUtilities", quietly=TRUE)) install.packages("neonUtilities")
if (!requireNamespace("BiocManager", quietly=TRUE)) install.packages("BiocManager")
if (!requireNamespace("rhdf5", quietly=TRUE)) BiocManager::install("rhdf5")
library(neonUtilities) # NEON dp04 downloader/extractor (zipsByProduct, stackEddy)
# Install metScanR from GitHub zipball (avoids some 404s from install_github)
get_metScanR <- function() {
if (requireNamespace("metScanR", quietly=TRUE)) return(TRUE)
if (!requireNamespace("remotes", quietly=TRUE)) install.packages("remotes")
zip_url <- "https://github.com/jaroberti/metScanR/archive/refs/heads/master.zip"  # public repo zip  (metScanR)  <-- see package repo
tf <- tempfile(fileext = ".zip")
ok <- try({
utils::download.file(zip_url, tf, mode="wb", quiet = TRUE)
install.packages(tf, repos = NULL, type = "source")
}, silent = TRUE)
requireNamespace("metScanR", quietly=TRUE)
}
has_metScanR <- isTRUE(get_metScanR())
if (has_metScanR) library(metScanR)
# ---- 1) Pick a region (or known site) & find co-located pair ------------
# Option A (recommended for the exercise): search near a known tower location.
# For Harvard Forest (NEON HARV / AmeriFlux US-xHA) approx coords:
lat0 <- 42.53
lon0 <- -72.18
if (has_metScanR) {
ms <- metScanR::siteFinder(Lat = lat0, Lon = lon0)   # find stations across networks near this point
nearby <- ms$site.data %>%
arrange(distance_km) %>%
filter(network %in% c("NEON","AMERIFLUX"))
message("Nearest NEON & AmeriFlux stations (top 10):")
print(head(nearby, 10))
# Select nearest NEON and AmeriFlux entries
neon_pick <- nearby %>% filter(network == "NEON") %>% slice(1)
amf_pick  <- nearby %>% filter(network == "AMERIFLUX") %>% slice(1)
# Extract station IDs (metScanR station_id is often usable for NEON code or AMF code)
neon_site <- as.character(neon_pick$station_id)  # NEON 4-letter code is typically present (e.g., "HARV")
amf_site  <- as.character(amf_pick$station_id)   # AmeriFlux site ID (e.g., "US-xHA")
# Apply a simple cleanup to pull a 4-letter NEON code if needed
neon_code <- stringr::str_match(neon_site, "([A-Z]{4})$")[,2]
if (is.na(neon_code)) neon_code <- neon_site
} else {
message("metScanR not available; falling back to a known co-located pair: US-xHA (AmeriFlux) ↔ HARV (NEON).")
amf_site  <- "US-xHA"
neon_code <- "HARV"
}
message(sprintf("Chosen pair -> AmeriFlux: %s, NEON: %s", amf_site, neon_code))
# ---- 2) Define overlapping time window ------------------------------
# Choose a window you expect both networks to have data for.
# Here we use "last ~6 months" as an example; adjust as needed.
t1 <- Sys.Date() - 1
t0 <- t1 - 180
# ---- 3) Download AmeriFlux BASE for the chosen AmeriFlux site -------
# You MUST provide your AmeriFlux account credentials:
user  <- "YOUR_AMERIFLUX_USERNAME"
# ---- 3) Download AmeriFlux BASE for the chosen AmeriFlux site -------
# You MUST provide your AmeriFlux account credentials:
user  <- "Ogonna"
email <- "Ogonnaeli@yahoo.com"
if (!nzchar(user) || !nzchar(email)) {
stop("Please set 'user' and 'email' to your AmeriFlux account credentials.")
}
amf_files <- amf_download_base(
user_id = user, user_email = email,
site_id = amf_site,
data_product = "BASE-BADM",
data_policy = "CCBY4.0",
agree_policy = TRUE,
intended_use = "education",
intended_use_text = "Exercise 5.7 NEON–AmeriFlux FC/H comparison",
out_dir = tempdir(), verbose = TRUE
)
amf_df <- amf_read_base(amf_files)
# AmeriFlux BASE timestamps are local standard time (no DST). Keep FC and H in the window.
amf_sub <- amf_df %>%
mutate(tstart_local = lubridate::ymd_hm(as.character(TIMESTAMP_START))) %>%   # parse local standard time
filter(tstart_local >= t0, tstart_local <= t1) %>%
select(tstart_local, FC_amf = FC, H_amf = H)
# ---- 4) Download NEON dp04 (HDF5) for the NEON code ------------------
# NEON DP4.00200.001; extract dp04 with stackEddy(); times are UTC; FC/H from turbulent components.
save_dir <- file.path(tempdir(), paste0("neon_dp04_", neon_code))
dir.create(save_dir, showWarnings = FALSE)
start_month <- format(t0, "%Y-%m")
end_month   <- format(t1, "%Y-%m")
zipsByProduct(
dpID = "DP4.00200.001",
site = neon_code,
startdate = start_month,
enddate   = end_month,
package = "basic",
savepath = save_dir,
check.size = FALSE
)
email <- "Ogonnaeli@yahoo.com"
source("C:/Users/Ope4/OneDrive - Northern Arizona University/Desktop/ACADEMIC_SEMSTER/SPRING 2026/INF_550/WEEK 5/WEEK_5B.R")
# -------------------------
# 0) Packages & setup
# -------------------------
base_pkgs <- c("dplyr","ggplot2","lubridate","stringr")
for (p in base_pkgs) if (!requireNamespace(p, quietly=TRUE)) install.packages(p)
lapply(base_pkgs, library, character.only = TRUE)
if (!requireNamespace("amerifluxr", quietly=TRUE)) install.packages("amerifluxr")
library(amerifluxr)   # amf_site_info(), amf_download_base(), amf_read_base
if (!requireNamespace("neonUtilities", quietly=TRUE)) install.packages("neonUtilities")
if (!requireNamespace("BiocManager", quietly=TRUE)) install.packages("BiocManager")
if (!requireNamespace("rhdf5", quietly=TRUE)) BiocManager::install("rhdf5")
library(neonUtilities) # zipsByProduct(), stackEddy()
# Try installing metScanR from GitHub ZIP (avoids some API 404 issues)
get_metScanR <- function() {
if (requireNamespace("metScanR", quietly=TRUE)) return(TRUE)
if (!requireNamespace("remotes", quietly=TRUE)) install.packages("remotes")
zip_url <- "https://github.com/jaroberti/metScanR/archive/refs/heads/master.zip"
tf <- tempfile(fileext = ".zip")
ok <- try({
utils::download.file(zip_url, tf, mode="wb", quiet=TRUE)
install.packages(tf, repos=NULL, type="source")
}, silent=TRUE)
requireNamespace("metScanR", quietly=TRUE)
}
has_metScanR <- isTRUE(get_metScanR())
if (has_metScanR) library(metScanR)
# -------------------------
# 1) Pick region & find co-located pair (metScanR → fallback)
# -------------------------
# Example search near Harvard Forest (you can change lat/lon)
lat0 <- 42.53
lon0 <- -72.18
if (has_metScanR) {
ms <- metScanR::siteFinder(Lat = lat0, Lon = lon0)       # Nearby stations across networks
nearby <- ms$site.data %>%
arrange(distance_km) %>%
filter(network %in% c("NEON","AMERIFLUX"))
message("Nearest NEON & AmeriFlux by metScanR (top 10):")
print(head(nearby, 10))
neon_pick <- nearby %>% filter(network=="NEON") %>% slice(1)
amf_pick  <- nearby %>% filter(network=="AMERIFLUX") %>% slice(1)
neon_site_raw <- as.character(neon_pick$station_id)
amf_site      <- as.character(amf_pick$station_id)
# Extract 4-letter NEON code if station_id includes extra text
neon_site <- stringr::str_match(neon_site_raw, "([A-Z]{4})$")[,2]
if (is.na(neon_site)) neon_site <- neon_site_raw
} else {
message("metScanR not available; using known co-located pair: US-xHA (AmeriFlux) ↔ HARV (NEON).")
amf_site  <- "US-xHA"
neon_site <- "HARV"
}
message(sprintf("Chosen pair → AmeriFlux: %s ; NEON: %s", amf_site, neon_site))
# -------------------------
# 2) Define overlapping time window
# -------------------------
# Choose a recent ~6-month window; adjust as needed
t1 <- Sys.Date() - 1
t0 <- t1 - 180
# -------------------------
# 3) AmeriFlux BASE download & subset (FC/H)
# -------------------------
# Supply your AmeriFlux credentials
user  <- "Ogonna"
email <- "Ogonnaeli@yahoo.com"
if (!nzchar(user) || !nzchar(email)) stop("Please set your AmeriFlux user/email.")
amf_files <- amf_download_base(
user_id = user, user_email = email,
site_id = amf_site,
data_product = "BASE-BADM",
data_policy = "CCBY4.0",
agree_policy = TRUE,
intended_use = "education",
intended_use_text = "Exercise 5.7 NEON–AmeriFlux FC/H comparison",
out_dir = tempdir(), verbose = TRUE
)
amf_df <- amf_read_base(amf_files)
# BASE timestamps are local standard time (no DST). Keep FC & H in [t0, t1]
amf_sub <- amf_df %>%
mutate(tstart_local = lubridate::ymd_hm(as.character(TIMESTAMP_START))) %>%
filter(tstart_local >= t0, tstart_local <= t1) %>%
select(tstart_local, FC_amf = FC, H_amf = H)
# -------------------------
# 4) NEON dp04 (DP4.00200.001) download & extract
# -------------------------
save_dir <- file.path(tempdir(), paste0("neon_dp04_", neon_site))
dir.create(save_dir, showWarnings = FALSE)
start_month <- format(t0, "%Y-%m")
end_month   <- format(t1, "%Y-%m")
# Try released files; if none, retry with include.provisional=TRUE
ok <- TRUE
tryCatch({
zipsByProduct(
dpID="DP4.00200.001", site=neon_site,
startdate=start_month, enddate=end_month,
package="basic", savepath=save_dir,
check.size=FALSE, include.provisional=FALSE
)
}, error=function(e){ message("Released dp04 not available; retrying with include.provisional=TRUE ..."); ok <<- FALSE })
if (!ok) {
zipsByProduct(
dpID="DP4.00200.001", site=neon_site,
startdate=start_month, enddate=end_month,
package="basic", savepath=save_dir,
check.size=FALSE, include.provisional=TRUE
)
}
flux_list <- stackEddy(file.path(save_dir, "filesToStack00200"), level="dp04", metadata=TRUE)
neon_dp04 <- flux_list[[neon_site]]
# -------------------------
# 5) Convert NEON UTC → AmeriFlux local standard time & select FC/H
# -------------------------
# Get UTC offset from AmeriFlux site metadata (if available), else fallback (e.g., EST = -5)
amf_sites <- amf_site_info()
utc_offset <- suppressWarnings(as.numeric(amf_sites$UTC_OFFSET[amf_sites$SITE_ID == amf_site]))
if (is.na(utc_offset)) {
message("UTC offset missing; defaulting to -5 (EST). Adjust if needed.")
utc_offset <- -5
}
